#!/usr/bin/env python3
"""
Script de test pour v√©rifier la connexion √† l'API Groq et le prompt syst√®me
"""

import os
from groq import Groq
from dotenv import load_dotenv

def load_system_prompt():
    """Charge le prompt syst√®me depuis le fichier"""
    try:
        with open("system_prompt.md", "r", encoding="utf-8") as f:
            return f.read()  # Utiliser le prompt complet avec ce mod√®le
    except FileNotFoundError:
        return """Vous √™tes un tuteur IA expert en physique-chimie pour les cycles 3 et 4 du syst√®me √©ducatif fran√ßais. 
        Votre mission est de guider l'√©l√®ve √©tape par √©tape pour qu'il construise lui-m√™me sa compr√©hension. 
        Utilisez la m√©thode socratique : posez des questions pour guider la r√©flexion plut√¥t que de donner les r√©ponses."""

def test_groq_connection():
    """Teste la connexion √† l'API Groq et le prompt syst√®me"""
    print("üß™ Test de connexion √† l'API Groq")
    print("=" * 50)
    
    # Charger les variables d'environnement
    load_dotenv()
    
    # V√©rifier la cl√© API
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        print("‚ùå Erreur: Cl√© API Groq manquante")
        print("   Veuillez cr√©er un fichier .env avec GROQ_API_KEY=your_key")
        return False
    
    print(f"‚úÖ Cl√© API trouv√©e: {api_key[:10]}...")
    
    # V√©rifier le prompt syst√®me
    system_prompt = load_system_prompt()
    if "system_prompt.md" in system_prompt:
        print("‚úÖ Fichier system_prompt.md trouv√© et charg√© (version compl√®te)")
    else:
        print("‚ö†Ô∏è Fichier system_prompt.md non trouv√©, utilisation du prompt par d√©faut")
    
    try:
        # Initialiser le client
        client = Groq(api_key=api_key)
        print("‚úÖ Client Groq initialis√©")
        
        # Test simple avec le mod√®le et le prompt syst√®me
        response = client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": "Vous √™tes un tuteur IA en physique-chimie. Utilisez la m√©thode socratique."
                },
                {
                    "role": "user",
                    "content": "Bonjour, je suis en 5e et j'ai du mal √† comprendre la diff√©rence entre une transformation physique et une transformation chimique. Peux-tu m'aider ?"
                }
            ],
            model="meta-llama/llama-4-scout-17b-16e-instruct",
            temperature=0.7,
            max_tokens=200
        )
        
        print("‚úÖ Test de g√©n√©ration avec prompt syst√®me r√©ussi")
        print(f"üìù R√©ponse du tuteur: {response.choices[0].message.content}")
        
        # V√©rifier que la r√©ponse suit la m√©thode socratique
        response_text = response.choices[0].message.content.lower()
        if any(keyword in response_text for keyword in ["question", "pense", "r√©fl√©chis", "d'apr√®s toi", "comment"]):
            print("‚úÖ La r√©ponse utilise bien la m√©thode socratique (questions guidantes)")
        else:
            print("‚ö†Ô∏è La r√©ponse pourrait ne pas suivre la m√©thode socratique")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors du test: {str(e)}")
        return False

def test_model_capabilities():
    """Teste les capacit√©s du mod√®le avec diff√©rents types de questions"""
    print("\nüß™ Test des capacit√©s du mod√®le")
    print("=" * 50)
    
    load_dotenv()
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        return False
    
    try:
        client = Groq(api_key=api_key)
        
        # Test avec une question de 6e
        print("üìö Test avec une question de 6e (Cycle 3):")
        response = client.chat.completions.create(
            messages=[
                {"role": "system", "content": "Vous √™tes un tuteur IA en physique-chimie. Utilisez la m√©thode socratique."},
                {"role": "user", "content": "Je suis en 6e et je ne comprends pas pourquoi l'eau change d'√©tat quand on la chauffe."}
            ],
            model="meta-llama/llama-4-scout-17b-16e-instruct",
            temperature=0.7,
            max_tokens=150
        )
        print(f"R√©ponse: {response.choices[0].message.content[:100]}...")
        
        # Test avec une question de 3e
        print("\nüìö Test avec une question de 3e (Cycle 4):")
        response = client.chat.completions.create(
            messages=[
                {"role": "system", "content": "Vous √™tes un tuteur IA en physique-chimie. Utilisez la m√©thode socratique."},
                {"role": "user", "content": "Je suis en 3e et j'ai du mal avec la formule de l'√©nergie cin√©tique Ec = 1/2 √ó m √ó v¬≤. Peux-tu m'expliquer ?"}
            ],
            model="meta-llama/llama-4-scout-17b-16e-instruct",
            temperature=0.7,
            max_tokens=150
        )
        print(f"R√©ponse: {response.choices[0].message.content[:100]}...")
        
        print("‚úÖ Tests des capacit√©s du mod√®le r√©ussis")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors des tests de capacit√©s: {str(e)}")
        return False

if __name__ == "__main__":
    success1 = test_groq_connection()
    success2 = test_model_capabilities()
    
    if success1 and success2:
        print("\nüéâ Tous les tests sont pass√©s ! Le tuteur IA est pr√™t √† √™tre utilis√©.")
        print("   Lancez 'streamlit run app.py' pour d√©marrer l'application.")
    else:
        print("\nüí• Des erreurs ont √©t√© d√©tect√©es. Veuillez les corriger avant de lancer l'application.") 